# -*- coding: utf-8 -*-
"""EyeIn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SRVuzLh3dk39TLEN17wxE-yOjkgqXeCe
"""

#step1: Install and Import Dependencies
!pip install kagglehub
import kagglehub
import os
import cv2
import glob
import matplotlib.pyplot as plt
import numpy as np
import shutil
from sklearn.model_selection import train_test_split
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import datasets
from PIL import Image

#step2: Download Dataset
print("Downloading dataset...")
dataset_path = kagglehub.dataset_download("jessicali9530/lfw-dataset")
print("Dataset downloaded to:", dataset_path)

from google.colab import drive
drive.mount('/content/drive')

INPUT_DIR = "/root/.cache/kagglehub/datasets/jessicali9530/lfw-dataset/versions/4"
OUTPUT_DIR = "/content/output_images"
os.makedirs(OUTPUT_DIR, exist_ok=True)

#step3: Collect and repare Face Images
print("Collecting and preparing face images...")
image_files = glob.glob(os.path.join(INPUT_DIR, '**', '*.jpg'), recursive=True)
image_files += glob.glob(os.path.join(INPUT_DIR, '**', '*.jpeg'), recursive=True)
image_files += glob.glob(os.path.join(INPUT_DIR, '**', '*.png'), recursive=True)

if not image_files:
    print("No images found in the dataset. Please verify the path.")
else:
    print(f"Found {len(image_files)} image files.")

#step4: Process Images
TARGET_WIDTH, TARGET_HEIGHT = 600, 400

print("Processing images...")
example_image_original = None
example_image_processed = None

def adjust_brightness_contrast(image, alpha=1.5, beta=30):
    return cv2.convertScaleAbs(image, alpha=alpha, beta=beta)

def apply_motion_blur(image, kernel_size=10):
    kernel = np.zeros((kernel_size, kernel_size))
    kernel[int((kernel_size - 1) / 2), :] = np.ones(kernel_size)
    kernel = kernel / kernel_size
    return cv2.filter2D(image, -1, kernel)

def add_gaussian_noise(image, mean=0, std=25):
    noise = np.random.normal(mean, std, image.shape).astype(np.uint8)
    noisy_image = cv2.add(image, noise)
    return noisy_image

for img_path in image_files:
    image = cv2.imread(img_path)

    if image is None:
        print(f"Skipping invalid file: {img_path}")
        continue

    # Step 1: Resize
    resized_image = cv2.resize(image, (TARGET_WIDTH, TARGET_HEIGHT), interpolation=cv2.INTER_AREA)

    # Step 2: Convert to Grayscale
    gray_image = cv2.cvtColor(resized_image, cv2.COLOR_BGR2GRAY)

    # Step 3: Adjust Brightness & Contrast
    enhanced_image = adjust_brightness_contrast(gray_image)

    # Step 4: Apply Motion Blur
    motion_blurred_image = apply_motion_blur(enhanced_image)

    # Step 5: Add Gaussian Noise
    # final_processed_image = add_gaussian_noise(motion_blurred_image)

    final_processed_image= motion_blurred_image

    # Save an example image before and after processing
    if example_image_original is None:
        example_image_original = resized_image
        example_image_processed = final_processed_image

    # Save Processed Image
    output_filename = os.path.splitext(os.path.basename(img_path))[0] + "_processed.jpg"
    output_path = os.path.join(OUTPUT_DIR, output_filename)
    cv2.imwrite(output_path, final_processed_image)

if example_image_original is not None and example_image_processed is not None:
    plt.figure(figsize=(10, 5))

    # Original Image
    plt.subplot(1, 2, 1)
    plt.imshow(cv2.cvtColor(example_image_original, cv2.COLOR_BGR2RGB))  # Convert BGR to RGB for correct display
    plt.title("Original Image")
    plt.axis("off")

    # Processed Image (Grayscale & Blurred)
    plt.subplot(1, 2, 2)
    plt.imshow(example_image_processed, cmap="gray")
    plt.title("Processed Image")
    plt.axis("off")

    plt.show()

print(f"Processing complete! Processed images saved in '{OUTPUT_DIR}'")

#step5: Split the Dataset
PROCESSED_DIR = OUTPUT_DIR
TRAIN_DIR = os.path.join(PROCESSED_DIR, "train")
VAL_DIR = os.path.join(PROCESSED_DIR, "val")
TEST_DIR = os.path.join(PROCESSED_DIR, "test")

# Create train, val, and test directories if they donâ€™t exist
for dir_path in [TRAIN_DIR, VAL_DIR, TEST_DIR]:
    os.makedirs(dir_path, exist_ok=True)

# Get all processed images
processed_images = glob.glob(os.path.join(PROCESSED_DIR, "*_processed.jpg"))

# Ensure we have images
if not processed_images:
    print("No processed images found. Check OUTPUT_DIR.")
else:
    print(f"Found {len(processed_images)} processed images.")

# Split into train (70%) and temp (30%)
train_files, temp_files = train_test_split(processed_images, test_size=0.3, random_state=42)

# Split temp into val (15%) and test (15%)
val_files, test_files = train_test_split(temp_files, test_size=0.5, random_state=42)

print(f"Train: {len(train_files)}, Validation: {len(val_files)}, Test: {len(test_files)}")

def move_files(file_list, destination_folder):
    for file_path in file_list:
        shutil.move(file_path, os.path.join(destination_folder, os.path.basename(file_path)))

# Move images
move_files(train_files, TRAIN_DIR)
move_files(val_files, VAL_DIR)
move_files(test_files, TEST_DIR)

print("Data successfully split and moved into train, validation, and test directories!")

print(f"Training images: {len(os.listdir(TRAIN_DIR))}")
print(f"Validation images: {len(os.listdir(VAL_DIR))}")
print(f"Testing images: {len(os.listdir(TEST_DIR))}")

#step 6: Conver Images to Tensor
IMAGE_SIZE = (224, 224)

transform = transforms.Compose([
    transforms.Resize(IMAGE_SIZE),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485], std=[0.229])
])

class CustomImageDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(".jpg")]

    def __len__(self):
        return len(self.image_files)

    def __getitem__(self, idx):
        img_path = self.image_files[idx]
        image = Image.open(img_path).convert("L")  # Convert to grayscale

        if self.transform:
            image = self.transform(image)

        return image

#step 7: Create DataLoaders
BATCH_SIZE = 32

# Create datasets
train_dataset = CustomImageDataset(TRAIN_DIR, transform=transform)
val_dataset = CustomImageDataset(VAL_DIR, transform=transform)
test_dataset = CustomImageDataset(TEST_DIR, transform=transform)

# Create DataLoaders
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

print(f"Train Dataset: {len(train_dataset)} images")
print(f"Validation Dataset: {len(val_dataset)} images")
print(f"Test Dataset: {len(test_dataset)} images")

import torch
import torch.nn as nn
import torch.optim as optim
import os
import glob
from torch.utils.data import Dataset, DataLoader
import torchvision.transforms as transforms
from torchvision import models
from sklearn.model_selection import train_test_split
from PIL import Image
import numpy as np
import random

# Set the random seed for reproducibility
seed = 42
random.seed(seed)  # For Python's random module
np.random.seed(seed)  # For NumPy
torch.manual_seed(seed)  # For PyTorch on CPU
torch.cuda.manual_seed(seed)  # For PyTorch on GPU
torch.cuda.manual_seed_all(seed)  # For all GPUs
torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior
torch.backends.cudnn.benchmark = False  # Disables non-deterministic algorithms


# Step 1: Define the CNN Model
class FaceRecognitionCNN(nn.Module):
    def __init__(self, num_classes):
        super(FaceRecognitionCNN, self).__init__()
        # We will use a simple CNN architecture
        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)
        self.fc1 = nn.Linear(128 * 28 * 28, 512)  # Adjust based on image size
        self.fc2 = nn.Linear(512, num_classes)
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = self.pool(self.relu(self.conv3(x)))
        x = x.view(-1, 128 * 28 * 28)  # Flatten the feature map
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

class CustomImageDataset(Dataset):
    def __init__(self, image_dir, transform=None):
        self.image_dir = image_dir
        self.transform = transform
        self.image_files = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(".jpg")]

        # Extract class names (person names) from filenames
        self.class_names = sorted(set(f.split('_')[0] for f in os.listdir(image_dir) if f.endswith(".jpg")))
        self.class_to_idx = {class_name: idx for idx, class_name in enumerate(self.class_names)}

    def __len__(self):
        return len(self.image_files)

    # def __getitem__(self, idx):
    #     img_path = self.image_files[idx]

    #     # Extract class name from the filename (before the first underscore)
    #     class_name = img_path.split('/')[-1].split('_')[0]  # Extract the first part of filename

    #     # Get the corresponding label using the class name
    #     label = self.class_to_idx[class_name]

    #     # Open and process the image
    #     image = Image.open(img_path).convert("L")  # Convert to grayscale

    #     if self.transform:
    #         image = self.transform(image)

    #     return image, label

    def __getitem__(self, idx):
    img_path = self.image_files[idx]

    # Convert the image to RGB (change from grayscale)
    image = Image.open(img_path).convert("RGB")  # Convert to RGB instead of grayscale

    if self.transform:
        image = self.transform(image)

    return image


# Step 3: Set up Image Transformations (Resize, Normalize)
transform = transforms.Compose([
    transforms.Resize((224, 224)),  # Resize the images to 224x224
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485], std=[0.229])  # For grayscale image normalization
])

# Step 4: Load Train, Validation, and Test Data
TRAIN_DIR = "/content/output_images/train"
VAL_DIR = "/content/output_images/val"
TEST_DIR = "/content/output_images/test"

# Create datasets and dataloaders
train_dataset = CustomImageDataset(TRAIN_DIR, transform=transform)
val_dataset = CustomImageDataset(VAL_DIR, transform=transform)
test_dataset = CustomImageDataset(TEST_DIR, transform=transform)

# Set up DataLoader for batching
BATCH_SIZE = 32
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False)
test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)

# Step 5: Instantiate the Model, Loss Function, and Optimizer
num_classes = len(train_dataset.class_names)  # Number of unique drivers
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

model = FaceRecognitionCNN(num_classes=num_classes).to(device)
criterion = nn.CrossEntropyLoss()  # For classification
optimizer = optim.Adam(model.parameters(), lr=0.0001)

# Step 6: Train the Model
num_epochs = 10
for epoch in range(num_epochs):
    model.train()  # Set the model to training mode
    running_loss = 0.0
    correct = 0
    total = 0

    for images, labels in train_loader:
        images, labels = images.to(device), labels.to(device)

        # Zero the gradients
        optimizer.zero_grad()

        # Forward pass
        outputs = model(images)
        loss = criterion(outputs, labels)

        # Backward pass and optimization
        loss.backward()
        optimizer.step()

        # Track the statistics
        running_loss += loss.item()
        _, predicted = torch.max(outputs, 1)
        total += labels.size(0)
        correct += (predicted == labels).sum().item()

    # Print statistics
    epoch_loss = running_loss / len(train_loader)
    epoch_accuracy = 100 * correct / total
    print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%")

# Step 7: Evaluate the Model
model.eval()  # Set the model to evaluation mode
val_correct = 0
val_total = 0

with torch.no_grad():
    for images, labels in val_loader:
        images, labels = images.to(device), labels.to(device)
        outputs = model(images)
        _, predicted = torch.max(outputs, 1)
        val_total += labels.size(0)
        val_correct += (predicted == labels).sum().item()

val_accuracy = 100 * val_correct / val_total
print(f"Validation Accuracy: {val_accuracy:.2f}%")

# Step 8: Save the Model
torch.save(model.state_dict(), "face_recognition_model.pth")

# Step 9: Predict the Driver's Identity in New Images
def predict_driver(image_path, model, class_to_idx, device):
    # Preprocess the image
    image = Image.open(image_path).convert("L")
    transform = transforms.Compose([
        transforms.Resize((224, 224)),
        transforms.ToTensor(),
        transforms.Normalize(mean=[0.485], std=[0.229])
    ])
    image = transform(image).unsqueeze(0).to(device)  # Add batch dimension and send to device

    # Make prediction
    model.eval()
    with torch.no_grad():
        outputs = model(image)
        _, predicted = torch.max(outputs, 1)

    predicted_class_idx = predicted.item()
    driver_name = [key for key, value in class_to_idx.items() if value == predicted_class_idx][0]
    print(f"Driver Identified: {driver_name}")

# Step 10: Test the prediction function
test_image_path = "/content/output_images/test/Aaron_Tippin_0001_processed.jpg"  # Replace with a test image
predict_driver(test_image_path, model, train_dataset.class_to_idx, device)